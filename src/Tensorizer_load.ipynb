{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub transformers accelerate diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensorizer import TensorSerializer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_ref = \"NousResearch/Llama-2-7b-hf\"\n",
    "# For less intensive requirements, swap above with the line below:\n",
    "# model_ref = \"EleutherAI/gpt-neo-125M\"\n",
    "model_name = model_ref.split(\"/\")[-1]\n",
    "# Change this to your S3 bucket.\n",
    "#s3_bucket = \"bucket\"\n",
    "#s3_uri = f\"s3://{s3_bucket}/{model_name}.tensors\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ref,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "serializer = TensorSerializer(\"model1.tensors\")\n",
    "serializer.write_module(model)\n",
    "serializer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from tensorizer import TensorDeserializer\n",
    "from tensorizer.utils import no_init_or_tensor, convert_bytes, get_mem_usage\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "model_ref = \"NousResearch/Llama-2-7b-hf\"\n",
    "# To run this at home, swap this with the line below for a smaller example:\n",
    "# model_ref = \"EleutherAI/gpt-neo-125M\"\n",
    "model_name = model_ref.split(\"/\")[-1]\n",
    "# Change this to your S3 bucket.\n",
    "load_loc = \"model1.tensors\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ref)\n",
    "\n",
    "# This ensures that the model is not initialized.\n",
    "with no_init_or_tensor():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "before_mem = get_mem_usage()\n",
    "\n",
    "# Lazy load the tensors from S3 into the model.\n",
    "start = time.time()\n",
    "deserializer = TensorDeserializer(load_loc, plaid_mode=True)\n",
    "deserializer.load_into_module(model)\n",
    "end = time.time()\n",
    "\n",
    "# Brag about how fast we are.\n",
    "total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n",
    "duration = end - start\n",
    "per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n",
    "after_mem = get_mem_usage()\n",
    "deserializer.close()\n",
    "print(f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\")\n",
    "print(f\"Memory usage before: {before_mem}\")\n",
    "print(f\"Memory usage after: {after_mem}\")\n",
    "\n",
    "# Tokenize and generate\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ref)\n",
    "eos = tokenizer.eos_token_id\n",
    "input_ids = tokenizer.encode(\n",
    "    \"Hello, Who is Presindent of USA?\", return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids, max_new_tokens=50, do_sample=True, pad_token_id=eos\n",
    "    )\n",
    "\n",
    "print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY USAGE and CPU USAGE\n",
    "import cProfile\n",
    "import psutil\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "from tensorizer import TensorDeserializer\n",
    "from tensorizer.utils import no_init_or_tensor, convert_bytes, get_mem_usage, get_gpu_name\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def load_and_run_model(csv_filename):\n",
    "    model_ref = \"/runpod-volume/model-13b\"\n",
    "    model_name = model_ref.split(\"/\")[-1]\n",
    "    load_loc = \"model2.tensors\"\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_ref)\n",
    "\n",
    "    with no_init_or_tensor():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    before_mem = get_mem_usage()\n",
    "\n",
    "    start = time.time()\n",
    "    deserializer = TensorDeserializer(load_loc, plaid_mode=True)\n",
    "    deserializer.load_into_module(model)\n",
    "    end = time.time()\n",
    "\n",
    "    total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n",
    "    duration = end - start\n",
    "    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n",
    "    after_mem = f\"Python Used RAM: {get_mem_usage()}\"\n",
    "    deserializer.close()\n",
    "\n",
    "    print(f\"Deserialized {total_bytes_str} in {duration:0.2f}s, {per_second}/s\")\n",
    "    print(f\"Memory usage before: {before_mem}\")\n",
    "    print(f\"Memory usage after: {after_mem}\")\n",
    "\n",
    "    plot_memory_usage(after_mem, end - start, model_name, get_gpu_name(), \"t-13b.csv\")\n",
    "\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_ref)\n",
    "    eos = tokenizer.eos_token_id\n",
    "    input_ids = tokenizer.encode(\n",
    "        \"Hello, Who is President of USA?\", return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids, max_new_tokens=50, do_sample=True, pad_token_id=eos\n",
    "        )\n",
    "\n",
    "    print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
    "    \n",
    "def plot_memory_usage(data_str, total_loading_time, input_directory, gpu_name, csv_filename):\n",
    "    \"\"\"\n",
    "    Plots memory usage and model details and saves them to a CSV file.\n",
    "\n",
    "    :param data_str: String containing memory usage data.\n",
    "    :param total_loading_time: Total time taken to load the model.\n",
    "    :param input_directory: Directory where the model is loaded from.\n",
    "    :param gpu_name: Name of the GPU used.\n",
    "    :param csv_filename: Filename to save the CSV data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parsing the data using regular expressions\n",
    "    parsed_values = re.findall(r'(\\d+,\\d+|\\d+)MiB', data_str)\n",
    "    parsed_values = [int(val.replace(',', '')) for val in parsed_values]\n",
    "\n",
    "    # Assigning values to categories\n",
    "    categories = ['CPU Maxrss', 'CPU Free', 'GPU Used', 'GPU Free', 'GPU Total', 'Torch Reserved', 'Torch Allocated']\n",
    "    values = parsed_values[:7]\n",
    "\n",
    "    # Saving data to CSV\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Category', 'Memory (MiB)', 'Total Loading Time (s)', 'Input Directory', 'GPU Name'])\n",
    "        for category, value in zip(categories, values):\n",
    "            csvwriter.writerow([category, value, total_loading_time, input_directory, gpu_name])\n",
    "\n",
    "    # Optional: Use Seaborn for better styling\n",
    "    sns.set_theme()\n",
    "\n",
    "    # Creating the bar chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.bar(categories, values, color=sns.color_palette(\"viridis\", len(categories)))\n",
    "\n",
    "    # Adding titles, labels, and annotations\n",
    "    plt.title('Memory Usage Metrics and Model Details')\n",
    "    plt.ylabel('Memory (MiB)')\n",
    "    plt.xlabel('Categories')\n",
    "\n",
    "    # Annotations in the top right corner\n",
    "    additional_info = f\"Total Loading Time: {total_loading_time} s\\nInput Directory: {input_directory}\\nGPU: {gpu_name}\"\n",
    "    plt.annotate(additional_info, xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "                 ha='right', va='top', fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"#cccccc\", facecolor=\"#ffffff\"))\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(\"memory_usage.png\")\n",
    "    plt.show()\n",
    "def profile_function(csv_filename):\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    load_and_run_model(csv_filename)\n",
    "\n",
    "    profiler.disable()\n",
    "    profiler.dump_stats('model_operations_profiling.prof')\n",
    "\n",
    "def monitor_resources(duration=20, interval=1):\n",
    "    cpu_usage = []\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < duration:\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=interval))\n",
    "    return cpu_usage\n",
    "\n",
    "def save_to_csv(data, filename, headers):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        for i, usage in enumerate(data):\n",
    "            writer.writerow([i, usage])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_filename = 'tensorhhh1.csv'\n",
    "\n",
    "    # Initialize CSV file with headers for memory usage logging\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Model Name', 'Memory Before (MiB)', 'Memory After (MiB)', 'Total Bytes', 'Loading Duration (s)'])\n",
    "\n",
    "    profile_function(csv_filename)\n",
    "\n",
    "    cpu_usage_data = monitor_resources()\n",
    "    save_to_csv(cpu_usage_data, 'cpu_usage_profile.csv', ['Time (s)', 'CPU Usage (%)'])\n",
    "\n",
    "    print(\"Profiling and resource monitoring complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
